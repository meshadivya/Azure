LTIMINDTREE Data Engineering Interview Answers
1. SQL/PySpark Query for Products with Exactly Two Transactions
SQL Solution (without window functions):

sql
WITH product_counts AS (
    SELECT product, COUNT(*) as trans_count
    FROM sales
    GROUP BY product
    HAVING COUNT(*) = 2
),
second_trans AS (
    SELECT s.*
    FROM sales s
    JOIN product_counts pc ON s.product = pc.product
    WHERE s.last_trans_date = (
        SELECT MAX(last_trans_date)
        FROM sales s2
        WHERE s2.product = s.product
        AND s2.last_trans_date < (
            SELECT MAX(last_trans_date)
            FROM sales s3
            WHERE s3.product = s.product
        )
    )
)
SELECT * FROM second_trans;

PySpark Solution (without window functions):

python
from pyspark.sql import functions as F

# Read sales data
sales_df = spark.table("sales")

# Get products with exactly 2 transactions
product_counts = sales_df.groupBy("product").agg(F.count("*").alias("trans_count"))
products_with_two = product_counts.filter(F.col("trans_count") == 2)

# Join to get all transactions for these products
two_trans_products = sales_df.join(
    products_with_two.select("product"),
    "product",
    "inner"
)

# Get max and second max dates per product
max_dates = two_trans_products.groupBy("product").agg(
    F.max("last_trans_date").alias("max_date")
)

second_max_dates = two_trans_products.join(
    max_dates,
    "product",
    "left"
).filter(
    F.col("last_trans_date") < F.col("max_date")
).groupBy("product").agg(
    F.max("last_trans_date").alias("second_max_date")
)

# Get details of second transaction
result = sales_df.join(
    second_max_dates,
    (sales_df.product == second_max_dates.product) & 
    (sales_df.last_trans_date == second_max_dates.second_max_date),
    "inner"
).select(sales_df["*"])

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
2. Azure Pipeline Design for File Movement
Pipeline Design:

Pipeline Components:

Lookup activity to read the Excel metadata file

ForEach activity to iterate through each file metadata

IfCondition activities to check file format

Copy activities for moving files to respective folders

Implementation Steps:

Use Azure Data Factory or Synapse Analytics

Create a parameterized pipeline

Read Excel metadata using Excel dataset

For each file in metadata:

Check if format is CSV → move to 'csv' folder

Check if format is Parquet → move to 'parquet' folder

Similar conditions for Avro/JSON if needed

Use Blob Storage or ADLS Gen2 as sink

Error Handling:

Implement logging for skipped files

Add retry logic for failed operations

Validate file existence before moving

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Loading CSV to Delta Table with Phone Formatting
Steps to Load CSV to Delta Table:

Read CSV file:

python
df = spark.read.option("header", "true").csv("/source/path/file.csv")
Transform phone number format:

python
from pyspark.sql.functions import regexp_replace

df_clean = df.withColumn(
    "Ph no",
    regexp_replace("Ph no", "^91\s*-\s*", "")
)
Write to Delta table:

python
df_clean.write.format("delta").saveAsTable("phone_numbers")
Alternative with Delta Lake:

python
# Create table if not exists
spark.sql("""
CREATE TABLE IF NOT EXISTS phone_numbers_delta (
    id INT,
    `Ph no` STRING
) USING DELTA
""")

# Insert transformed data
df_clean.write.format("delta").mode("append").saveAsTable("phone_numbers_delta")

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
4. SCD Type 2 Implementation in Databricks
SCD Type 2 Definition:
Slowly Changing Dimension Type 2 tracks historical data by:

Adding new records for changes

Maintaining effective date ranges

Flagging current records

Implementation Steps in Databricks:

Identify changes:

python
changes = new_data.join(
    existing_data,
    (new_data.key == existing_data.key) & 
    (new_data.hash != existing_data.hash),
    "inner"
)
Expire old records:

python
expired_records = existing_data.join(
    changes.select("key"),
    "key",
    "inner"
).withColumn("end_date", current_date())
Insert new records:

python
new_records = changes.withColumn("start_date", current_date()) \
                    .withColumn("end_date", lit(None)) \
                    .withColumn("current_flag", lit(True))
Merge operation:

python
(deltaTable.alias("target")
 .merge(
     updates.alias("source"),
     "target.key = source.key AND target.current_flag = true"
 )
 .whenMatchedUpdate(
     set = {
         "current_flag": "false",
         "end_date": "current_date()"
     }
 )
 .whenNotMatchedInsertAll()
 .execute())

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Fact vs Dimension Tables
Fact Tables:

Contain measurable, quantitative data (metrics/facts)

Foreign keys to dimension tables

Typically numeric and additive values

Example: Sales amount, quantity sold

Dimension Tables:

Contain descriptive attributes

Provide context for facts

Text-heavy with many attributes

Example: Product details, customer information

Which changes more frequently:

Fact tables change more frequently as they record transactions/events

Dimension tables change less often but may have SCD implementations
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
6. Snowflake vs Star Schema
Star Schema:

Single fact table connected to multiple dimension tables

Denormalized dimension tables

Simple structure, better for simple queries

Faster for most queries but uses more storage

Snowflake Schema:

Fact table connected to normalized dimension tables

Dimension tables are split into multiple related tables

More complex structure, better for storage efficiency

Requires more joins but uses less storage
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
7. PySpark Anti-Join Code Snippet
Anti-Join Implementation:

python
# Method 1: Using join with filter
anti_join_df = df1.join(
    df2,
    df1["key"] == df2["key"],
    "left"
).filter(
    df2["key"].isNull()
).select(df1["*"])

# Method 2: Using subtract
anti_join_df = df1.select("key").subtract(df2.select("key"))
anti_join_df = df1.join(anti_join_df, "key", "inner")

# Method 3: Using SQL syntax
df1.createOrReplaceTempView("table1")
df2.createOrReplaceTempView("table2")
anti_join_df = spark.sql("""
    SELECT t1.* 
    FROM table1 t1
    LEFT ANTI JOIN table2 t2
    ON t1.key = t2.key
""")
